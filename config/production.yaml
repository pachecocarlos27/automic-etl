# Automic ETL - Production Configuration
# Copy to settings.yaml and set environment variables for deployment

lakehouse:
  name: "${AUTOMIC_LAKEHOUSE_NAME:production_lakehouse}"
  description: "Production AI-Augmented Data Lakehouse"

# Storage provider configuration
storage:
  provider: "${AUTOMIC_STORAGE_PROVIDER:aws}"

  aws:
    bucket: "${AWS_LAKEHOUSE_BUCKET}"
    region: "${AWS_REGION:us-east-1}"
    # Use IAM roles in production instead of static credentials
    access_key_id: null
    secret_access_key: null
    role_arn: "${AWS_ROLE_ARN}"
    endpoint_url: null

  gcp:
    bucket: "${GCP_LAKEHOUSE_BUCKET}"
    project_id: "${GCP_PROJECT_ID}"
    # Use workload identity in production
    credentials_file: null

  azure:
    container: "${AZURE_LAKEHOUSE_CONTAINER}"
    storage_account: "${AZURE_STORAGE_ACCOUNT}"
    connection_string: null
    # Use managed identity in production
    use_managed_identity: true

# Apache Iceberg configuration - Production
iceberg:
  catalog:
    # Use AWS Glue for production AWS deployments
    type: "${AUTOMIC_CATALOG_TYPE:glue}"
    name: "automic_production"

    # For Glue catalog (recommended for AWS)
    # Requires: AWS_REGION set

    # For REST catalog (recommended for multi-cloud)
    # uri: "${ICEBERG_REST_CATALOG_URI}"

    # For Hive Metastore
    # uri: "thrift://${HIVE_METASTORE_HOST}:9083"

  warehouse: "warehouse/"

  table_defaults:
    format_version: 2
    write:
      target_file_size_bytes: 536870912  # 512MB
      parquet:
        compression: "zstd"
        compression_level: 3

# Medallion architecture - Production
medallion:
  bronze:
    path: "bronze/"
    description: "Raw data layer - production"
    retention_days: 90
    partition_by: ["_ingestion_date"]

  silver:
    path: "silver/"
    description: "Cleaned and validated data layer"
    retention_days: 365
    partition_by: ["_processing_date"]

  gold:
    path: "gold/"
    description: "Business-level aggregations and ML-ready data"
    retention_days: null
    partition_by: []

# LLM configuration - Production
llm:
  provider: "${AUTOMIC_LLM_PROVIDER:anthropic}"
  model: "${AUTOMIC_LLM_MODEL:claude-sonnet-4-20250514}"
  api_key: "${ANTHROPIC_API_KEY}"

  # Rate limiting for production
  temperature: 0.0  # Deterministic for production
  max_tokens: 4096
  timeout: 60  # Lower timeout in production

  # Features
  features:
    schema_inference: true
    entity_extraction: true
    data_classification: true
    anomaly_detection: true
    query_building: true

# Extraction configuration - Production
extraction:
  default_mode: "incremental"

  batch:
    size: 500000  # Larger batches in production
    parallel_workers: 8  # More workers

  incremental:
    watermark_strategy: "timestamp"
    watermark_column: "updated_at"
    lookback_window: "2 hours"  # Longer lookback for reliability

  cdc:
    enabled: "${AUTOMIC_CDC_ENABLED:false}"
    method: "timestamp"

# Data quality - Production (strict)
data_quality:
  enabled: true

  checks:
    null_check: true
    duplicate_check: true
    schema_validation: true
    range_validation: true
    referential_integrity: true  # Enable in production

  on_failure: "quarantine"  # Strict in production

  quarantine:
    enabled: true
    path: "quarantine/"

# Connectors - Production
connectors:
  timeout: 60  # Longer timeout

  retry:
    max_attempts: 5  # More retries
    backoff_factor: 2

  databases:
    pool_size: 20  # Larger pool
    max_overflow: 30

  apis:
    rate_limit: 500
    concurrent_requests: 20

# Logging - Production
logging:
  level: "${AUTOMIC_LOG_LEVEL:INFO}"
  format: "json"
  file: "/var/log/automic/automic-etl.log"

# Metrics - Production
metrics:
  enabled: true
  provider: "${AUTOMIC_METRICS_PROVIDER:prometheus}"

# Pipeline - Production
pipeline:
  mode: "parallel"
  max_parallel_jobs: 8

  checkpoint:
    enabled: true
    interval: 5000

  schedule: null

# =============================================================================
# Apache Airflow Integration - Production
# =============================================================================
airflow:
  # Airflow REST API configuration
  base_url: "${AIRFLOW_BASE_URL:http://localhost:8080}"
  api_path: "/api/v1"

  # Authentication
  username: "${AIRFLOW_USERNAME}"
  password: "${AIRFLOW_PASSWORD}"

  # DAG configuration
  dags_folder: "${AIRFLOW_DAGS_FOLDER:/opt/airflow/dags}"
  default_schedule: "@daily"
  default_catchup: false

  # Default arguments for all DAGs
  default_args:
    owner: "automic-etl"
    retries: 3
    retry_delay_minutes: 5
    email_on_failure: true
    email_on_retry: false
    depends_on_past: false

  # Pool configuration
  pools:
    bronze_pool: 8
    silver_pool: 6
    gold_pool: 4
    llm_pool: 2  # Limit concurrent LLM operations

  # Connection IDs
  connections:
    lakehouse: "automic_lakehouse"
    llm: "automic_llm"
    notification: "automic_notifications"

# =============================================================================
# Agentic Configuration - Production
# =============================================================================
agentic:
  enabled: true

  # Agent decision thresholds
  decision:
    confidence_threshold: 0.8  # High confidence required
    auto_approve_low_risk: true
    auto_approve_medium_risk: false
    require_approval_high_risk: true

  # Self-healing configuration
  self_healing:
    enabled: true
    max_auto_retries: 3
    escalation_threshold: 2  # Escalate after 2 failures

  # Optimization settings
  optimization:
    auto_apply_low_risk: true
    schedule_analysis_interval: "24h"
    performance_window_days: 14

# =============================================================================
# Security - Production
# =============================================================================
security:
  # Authentication
  auth:
    enabled: true
    provider: "${AUTOMIC_AUTH_PROVIDER:jwt}"
    jwt_secret: "${AUTOMIC_JWT_SECRET}"
    jwt_algorithm: "HS256"
    token_expiry_hours: 24

  # API security
  api:
    rate_limit_per_minute: 1000
    max_request_size_mb: 100
    allowed_origins: "${AUTOMIC_ALLOWED_ORIGINS:*}"

  # Encryption
  encryption:
    at_rest: true
    in_transit: true
    kms_key_id: "${AUTOMIC_KMS_KEY_ID}"

# =============================================================================
# Multi-tenant - Production
# =============================================================================
multi_tenant:
  enabled: "${AUTOMIC_MULTI_TENANT:false}"

  # Isolation
  isolation_mode: "schema"  # schema, database, or storage

  # Limits per company (can be overridden per tenant)
  default_limits:
    max_pipelines: 100
    max_storage_gb: 1000
    max_api_calls_per_day: 100000
    max_llm_tokens_per_day: 1000000
    max_users: 50
    max_concurrent_jobs: 10

# =============================================================================
# Notifications - Production
# =============================================================================
notifications:
  enabled: true

  # Channels
  email:
    enabled: "${AUTOMIC_EMAIL_ENABLED:false}"
    smtp_host: "${SMTP_HOST}"
    smtp_port: "${SMTP_PORT:587}"
    smtp_user: "${SMTP_USER}"
    smtp_password: "${SMTP_PASSWORD}"
    from_address: "${AUTOMIC_EMAIL_FROM:noreply@automic-etl.io}"

  slack:
    enabled: "${AUTOMIC_SLACK_ENABLED:false}"
    webhook_url: "${SLACK_WEBHOOK_URL}"
    channel: "${SLACK_CHANNEL:#data-alerts}"

  pagerduty:
    enabled: "${AUTOMIC_PAGERDUTY_ENABLED:false}"
    routing_key: "${PAGERDUTY_ROUTING_KEY}"

  # Alert thresholds
  alerts:
    pipeline_failure: true
    data_quality_warning: true
    sla_breach: true
    resource_threshold_percent: 80

# =============================================================================
# Observability - Production
# =============================================================================
observability:
  # Tracing
  tracing:
    enabled: "${AUTOMIC_TRACING_ENABLED:false}"
    provider: "opentelemetry"  # opentelemetry, jaeger, zipkin
    endpoint: "${OTEL_EXPORTER_OTLP_ENDPOINT}"
    sample_rate: 0.1

  # Health checks
  health:
    enabled: true
    endpoint: "/health"
    include_details: false  # Don't expose details in production

  # Profiling
  profiling:
    enabled: "${AUTOMIC_PROFILING_ENABLED:false}"
    provider: "pyroscope"
